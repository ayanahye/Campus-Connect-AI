{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08CJHqgU4QVR"
      },
      "source": [
        "Updated notebook - main changes: revert to separating each category into its own collection, implement 2 step pipeline (categorize, then retrieve from those relevant collections). Somehow, takes a substantial amount of time now (ie 7 queries in 17 minutes) (using general qa set and some other random set of data), to discuss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNlcwxKR9gXt"
      },
      "source": [
        "Install some dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0wlrXoS1EP-"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U accelerate==0.27.1\n",
        "!pip install -q -U datasets==2.17.0\n",
        "!pip install -q -U transformers==4.38.1\n",
        "!pip install langchain sentence-transformers chromadb langchainhub\n",
        "!pip install llama-cpp-python\n",
        "!pip install langchain-community langchain-core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaDCl4Jj-YCQ"
      },
      "source": [
        "Define Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymTffeGN4EH-"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "model_path = \"mistral-7b-instruct-v0.2.Q4_0.gguf\"\n",
        "model = Llama(model_path=model_path, n_ctx=2048, n_threads=8, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jGq9g41dv3ru"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/fz/m3b664vj5hq4kdmwr645k9_h0000gn/T/ipykernel_9517/1223792325.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import chromadb\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# persistent client to interact w chroma vector store\n",
        "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# create collections for each data (for testing rn)\n",
        "collection = client.get_or_create_collection(name=\"combined_docs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBA-ljP5-bOR"
      },
      "source": [
        "Define Data Sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVLl-QTT61Aw",
        "outputId": "22d88b44-33d8-4f08-c0d1-2d480c1944ba"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "import uuid\n",
        "import os\n",
        "\n",
        "file_names = [\n",
        "    \"study_permit_general\", \"work_permit_student_general\", \"work-study-data-llm\",\n",
        "    \"vancouver_transit_qa_pairs\", \"permanent_residence_student_general\", \"data-with-sources\",\n",
        "    \"faq_qa_pairs_general\", \"hikes_qa\", \"sfu-faq-with-sources\", \"sfu-housing-with-sources\",\n",
        "    \"sfu-immigration-faq\", \"park_qa_pairs-up\", \"cultural_space_qa_pairs_up\",\n",
        "    \"qa_pairs_food\", \"qa_pairs_year_and_month_avg\", \"qa_pairs_sfu_clubs\"\n",
        "]\n",
        "\n",
        "collections = {}\n",
        "batch_size = 32\n",
        "\n",
        "def process_file(file):\n",
        "    try:\n",
        "        path = f'../Data/{file}.csv'\n",
        "        if not os.path.exists(path):\n",
        "            return f\"{file} skipped (file not found).\"\n",
        "\n",
        "        df = pd.read_csv(path, usecols=lambda col: col.lower() in {\"question\", \"answer\"})\n",
        "        df.columns = df.columns.str.lower()\n",
        "\n",
        "        if \"question\" not in df.columns or \"answer\" not in df.columns:\n",
        "            return f\"{file} skipped (missing question/answer columns).\"\n",
        "\n",
        "        df = df.drop_duplicates(subset=\"question\")\n",
        "        df[\"text\"] = df[\"question\"].fillna('') + ' ' + df[\"answer\"].fillna('')\n",
        "        unique_texts = list(set(df[\"text\"].dropna().tolist()))\n",
        "\n",
        "        collection = client.get_or_create_collection(name=file)\n",
        "        for i in range(0, len(unique_texts), batch_size):\n",
        "            batch = unique_texts[i:i + batch_size]\n",
        "            embeddings = embedding_model.embed_documents(batch)\n",
        "            ids = [str(uuid.uuid4()) for _ in batch]\n",
        "            collection.add(ids=ids, embeddings=embeddings, documents=batch)\n",
        "\n",
        "        collections[file] = collection\n",
        "        return f\"{file}: Loaded {len(unique_texts)} docs.\"\n",
        "    except Exception as e:\n",
        "        return f\"{file}: Error - {e}\"\n",
        "\n",
        "# parallelogram\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
        "    results = list(executor.map(process_file, file_names))\n",
        "\n",
        "for result in results:\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r976Yrgqdvjk"
      },
      "outputs": [],
      "source": [
        "# define cat to collection mapping\n",
        "# motivation: takes wayyyy too long now -> dataset size trippled and time grew exponentially...\n",
        "# now takes around 2 mins on avg for each response..compared to 40 seconds previously..\n",
        "\n",
        "collection_map = {\n",
        "    \"study-permit\": \"study_permit_general\",\n",
        "    \"work-permit\": \"work_permit_student_general\",\n",
        "    \"work-study\": \"work-study-data-llm\",\n",
        "    \"transit\": \"vancouver_transit_qa_pairs\",\n",
        "    \"permanent residence\": \"permanent_residence_student_general\",\n",
        "    \"health\": \"data-with-sources\",\n",
        "    \"faq\": \"faq_qa_pairs_general\",\n",
        "    \"hiking\": \"hikes_qa\",\n",
        "    \"sfu-faq\": \"sfu-faq-with-sources\",\n",
        "    \"housing\": \"sfu-housing-with-sources\",\n",
        "    \"immigration\": \"sfu-immigration-faq\",\n",
        "    \"parks\": \"park_qa_pairs-up\",\n",
        "    \"culture\": \"cultural_space_qa_pairs_up\",\n",
        "    \"food\": \"qa_pairs_food\",\n",
        "    \"expenditure\": \"qa_pairs_year_and_month_avg\",\n",
        "    \"clubs\": \"qa_pairs_sfu_clubs\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn2LQiJq_k9t"
      },
      "source": [
        "Function to now match for releveant document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TEulNQZw8mDo"
      },
      "outputs": [],
      "source": [
        "def get_relevant_documents(query, categories, n_results=3):\n",
        "    all_results = []\n",
        "    query_embedding = embedding_model.embed_documents([query])[0]\n",
        "\n",
        "    for category in categories:\n",
        "        collection_name = collection_map[category]\n",
        "        if collection_name in collections:\n",
        "            try:\n",
        "                result = collections[collection_name].query(\n",
        "                    query_embeddings=[query_embedding],\n",
        "                    n_results=n_results\n",
        "                )\n",
        "                docs = result.get(\"documents\", [[]])[0]\n",
        "                sims = result.get(\"distances\", [[]])[0]\n",
        "\n",
        "                all_results.extend(zip(docs, sims))\n",
        "            except Exception as e:\n",
        "                print(f\"error querying {collection_name}: {e}\")\n",
        "\n",
        "    all_results = sorted(all_results, key=lambda x: x[1])\n",
        "\n",
        "    return all_results[:n_results]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjivdEV5fPxq"
      },
      "source": [
        "### Classify Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xVTQznmhfN9V"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import difflib\n",
        "\n",
        "valid_categories = list(collection_map.keys())\n",
        "fallback_category = \"faq\"\n",
        "\n",
        "def classify_query(query):\n",
        "    category_prompt = f\"\"\"\n",
        "    You are a classifier for a Q&A system for international students in British Columbia.\n",
        "    Pick ONLY from this list of category names (copy them exactly, case-insensitive), and return up to 3 relevant ones (comma-separated):\n",
        "\n",
        "    {\", \".join(valid_categories)}\n",
        "\n",
        "    Query: \"{query}\"\n",
        "\n",
        "    Return only the category name(s) as a comma-separated string.\n",
        "    \"\"\"\n",
        "\n",
        "    response = model(category_prompt, max_tokens=50, temperature=0.1)[\"choices\"][0][\"text\"].strip().lower()\n",
        "    print(\"Raw out:\", response)\n",
        "    \n",
        "\n",
        "    tokens = [t.strip() for t in response.split(\",\")]\n",
        "\n",
        "    matched = []\n",
        "    for token in tokens:\n",
        "        closest = difflib.get_close_matches(token, valid_categories, n=1, cutoff=0.7)\n",
        "        if closest and closest[0] not in matched:\n",
        "            matched.append(closest[0])\n",
        "        if len(matched) == 3:\n",
        "            break\n",
        "\n",
        "    if fallback_category not in matched:\n",
        "      matched.append(fallback_category)\n",
        "        \n",
        "    return matched[:3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV_h8UJa_pGj"
      },
      "source": [
        "Generate Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMTYliNl8onj"
      },
      "outputs": [],
      "source": [
        "def generate_answer(query):\n",
        "    categories = classify_query(query)\n",
        "    print(f\"Categories {categories}\\n\")\n",
        "    relevant_documents = get_relevant_documents(query, categories)\n",
        "\n",
        "    if not relevant_documents:\n",
        "        return {\n",
        "            \"Response\": \"Sorry, no relevant documents found.\"\n",
        "        }\n",
        "\n",
        "    #relevant_documents = list(set(relevant_documents))\n",
        "\n",
        "    seen = set()\n",
        "    unique_docs = []\n",
        "    for doc, sim in relevant_documents:\n",
        "        if doc not in seen:\n",
        "            seen.add(doc)\n",
        "            unique_docs.append((doc, sim))\n",
        "\n",
        "    print(\"Relevant Documents with Similarity Scores:\")\n",
        "    for doc, sim in unique_docs:\n",
        "        print(f\"Similarity: {sim:.4f}\\nDoc: {doc}\\n\")\n",
        "\n",
        "    relevant_texts = \"\\n\\n\".join([doc for doc, _ in unique_docs])\n",
        "    \n",
        "    ## additional prompts\n",
        "    hike_prompt = f\"\"\"\n",
        "        INSTRUCTIONS:\n",
        "            1. Convert structured information about the hike into a short, friendly paragraph using natural language. Do not repeat numbers or use formatting from the source.\n",
        "            2. If they ask about hiking information, only answer with required information. Users can ask for more information if needed.\n",
        "            3. When asked for a particular type of hike, find it instead of saying that one would not work in the category they asked for.\n",
        "            4. Do NOT list trail attributes or stats (like “Distance: 3.1 km, Elevation: 789 m”). Instead, describe them in context (e.g., “a steep 3 km trail with a tough 789 m climb”).\n",
        "            5. Avoid repeating exact numbers unless essential (e.g., elevation gain is helpful, but don't dump all stats).\n",
        "    \"\"\"\n",
        "    \n",
        "    parks_prompt = f\"\"\" \n",
        "        INSTRUCTIONS:\n",
        "            1. Convert structured information about the park into a short, friendly paragraph using natural language. Do not repeat numbers or use formatting from the source.\n",
        "            2. Provide only necessary information that will allow the user to enjoy the park.\n",
        "                - Feel free to tell them about logisitical information if asked.\n",
        "    \"\"\"\n",
        "    \n",
        "    food_prompt = f\"\"\" \n",
        "        INSTRUCTIONS:\n",
        "            1. Convert structured food and dining information into a friendly, helpful paragraph. Do not copy the question or use list formatting.\n",
        "            2. Only answer what the user asked. Do NOT add information that wasn't requested.\n",
        "            3. Describe details in a natural way (e.g., “open 24/7 during the semester” instead of “Hours: 24/7”).\n",
        "            4. Mention unique features only when they help clarify the user's question.\n",
        "            5. If a specific venue or program is asked about (e.g., a café, meal plan, or food station), describe it clearly in context.\n",
        "            6. If the question can't be answered from the data, respond with: “I'm sorry, I don't have that information. Please check the official SFU Food website.”\n",
        "            7. Provide the official link when available and relevant to the answer.\n",
        "            8. Do NOT list menu items, prices, or square footage unless directly relevant to the user's question.\n",
        "            9. Only provide food information that is relevant. If they ask for some place that serves a chicken sandwich do not provide information to a vegan place.\n",
        "    \"\"\"\n",
        "\n",
        "    ## activities general covers how to answer general parks, hikes, food, clubs, cultural related questions \n",
        "    activities_general = f\"\"\" \n",
        "        INSTRUCTIONS:\n",
        "            1. If they ask for suggestions, provide 2 to 3 suggestions.\n",
        "            2. Do NOT list all information. Instead describe them in context \n",
        "            3. Provide accuracte suggestions, NOT suggestions of things that will not work for what they want.\n",
        "            4. Convert structured information about the activity into a short, friendly paragraph using natural language. Do not repeat formatting from the source.\n",
        "    \"\"\"\n",
        "    \n",
        "    ## permits prompt - covers ways to answer immigration, study permits, work permits, and permanent residence related questions \n",
        "    permits_prompt = f\"\"\"\n",
        "        INSTRUCTIONS:\n",
        "            1. When given a specific question with many possible answers, you can ask for more specific information.\n",
        "                - if they are not asking for an extension do not provide information in regards to an extension of a permit.\n",
        "            2. Only answer with information provided \n",
        "                - Information should NOT be guessed and do NOT add extra information\n",
        "            3. If the answer is not in the dataset, respond with: \"I'm sorry, I don't have that information. Please check the official IRCC website for more details.\"\n",
        "            4. If it is helpful, provide the link and a description about it.\n",
        "            5. Do NOT list all information. Instead describe them in context \n",
        "            6. If the answer depends on a specific condition explain those clearly.\n",
        "            7. Do NOT make assumptions about the user's situation. \n",
        "    \"\"\"\n",
        "    \n",
        "    housing_prompt = f\"\"\" \n",
        "        INSTRUCTIONS:\n",
        "            1. Convert structured information about SFU or general student housing into a short, friendly paragraph using natural language. Do not repeat formatting or list prices unless helpful for context.\n",
        "            2. Focus on what matters to the student: location, room types, meal plans, how to apply, and support available.\n",
        "            3. Only mention costs in a general way (e.g., \"starts around $4,000 per term\") unless the user explicitly asks for detailed pricing.\n",
        "            4. If information varies (e.g., by room type or campus), explain this clearly but briefly.\n",
        "            5. If the user asks a specific housing question and the answer depends on certain conditions (e.g., term length, student status), explain those conditions clearly and simply.\n",
        "            6. If the answer is not known or not in the data, respond with: \"I’m sorry, I don’t have that information. Please check the SFU Housing website for details.\"\n",
        "            7. Do NOT dump full lists of buildings, prices, or amenities. Summarize and keep it conversational.\n",
        "            8. If the information is specific to SFU, make sure you say it to be clear.\n",
        "    \"\"\"\n",
        "    \n",
        "    transit_prompt = f\"\"\" \n",
        "        INSTRUCTIONS:\n",
        "            1. Convert structured information about public transit into a short, friendly paragraph using natural language.\n",
        "            2. Do NOT list statistics or technical formatting (like route numbers or fare charts) unless directly relevant to the user's question.\n",
        "            3. Summarize relevant transit options clearly — describe them in context (e.g., “a quick SkyTrain ride from downtown to the airport”).\n",
        "            4. Provide only what the user needs to understand how to get around or plan their trip.\n",
        "            5. If the user is asking for directions, give a general summary of how they might travel.\n",
        "            6. If the question is about fares, schedules, or route planning and the exact info is not available, tell the user to check the TransLink website and briefly explain what they can find there.\n",
        "            7. Do NOT guess or make up transit information.\n",
        "            8. If the information is not in the source, say “I’m sorry, I don’t have that information. You can check the official TransLink site for more details.”\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    ## main rag prompt - how to answer general questions \n",
        "    rag_prompt = f\"\"\"\n",
        "    You are a helpful, friendly assistant for international students new to British Columbia, Canada.\n",
        "\n",
        "    Below are some reference documents that may be relevant to the user's question:\n",
        "    {relevant_texts}\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    1. If the user's query is just a greeting (like \"hello\", \"hi\", \"what's up\"):\n",
        "       - Respond with a single brief friendly greeting\n",
        "       - Offer to help with questions about studying or living in BC\n",
        "       - Do NOT include ANY information from the reference documents\n",
        "       - Do NOT create additional answers beyond answering their original question\n",
        "\n",
        "    2. If the user is asking for information:\n",
        "       - Be friendly and answer based ONLY on the reference documents if relevant\n",
        "       - Summarize the necessary information into a couple sentences.\n",
        "       - Do NOT create additional questions and answers beyond answering their original question\n",
        "       - Limit your entire response to no more than 3 concise sentences when possible. Do not create long multi-line answers.\n",
        "       - If the documents don't provide sufficient information, say \"I don't have enough information to answer that. Please refer to official sources.\"\n",
        "       - Ask for more information when there are multiple scenarios in the documents.\n",
        "       - If they ask things like \"can I\", \"will I\", \"how can I\" feel free to ask follow up questions if you don't how to answer with the information provided. Do not just assume.\n",
        "    \n",
        "    3. IMPORTANT: Never generate additional content beyond answering the user's question. Do NOT number or bullet your points. Always use natural sentences and group similar information together where possible.\n",
        "    \n",
        "    Do NOT restate the user's question (like: User question: ..., Your response\\n...) or include any of the reference documents in your answer.\n",
        "    User question: {query}\n",
        "\n",
        "    Your response (just the answer, no preamble):\n",
        "    \"\"\"\n",
        "    \n",
        "    # adding the category specific prompting to main if necessary\n",
        "    for category in categories:\n",
        "        if category == \"hiking\" or category == \"parks\" or category == \"food\" or category == \"cultural\" or category == \"clubs\":\n",
        "            rag_prompt += \"\\n\" + activities_general\n",
        "            \n",
        "        if category == \"hiking\":\n",
        "            rag_prompt += \"\\n\" + hike_prompt\n",
        "            \n",
        "        if category == \"parks\":\n",
        "            rag_prompt += \"\\n\" + parks_prompt\n",
        "        \n",
        "        if category == \"food\":\n",
        "            rag_prompt += \"\\n\" + food_prompt\n",
        "            \n",
        "        if category == \"study permit\" or category == \"work permit\" or category == \"immigration\" or category == \"permanent residence\":\n",
        "            rag_prompt += \"\\n\" + permits_prompt\n",
        "            \n",
        "        if category == \"housing\":\n",
        "            rag_prompt += \"\\n\" + housing_prompt\n",
        "        \n",
        "        if category == \"transit\":\n",
        "            rag_prompt += \"\\n\" + transit_prompt\n",
        "       \n",
        "    response_after_rag = model(rag_prompt, max_tokens=300, temperature=0.1)[\"choices\"][0][\"text\"]\n",
        "    # response_after_rag = model(rag_prompt, max_tokens=300, temperature=0.1)\n",
        "\n",
        "    return {\n",
        "        \"Response\": response_after_rag\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxtEFTiK_q8E"
      },
      "source": [
        "Example Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "answer = generate_answer(\"what if I only have one parent when applying for my study permit?\")\n",
        "print(answer[\"Response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install evaluate\n",
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EVDilNoM8sU8",
        "outputId": "20b5ccda-84b1-4213-d257-dcd59e8e761c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from evaluate import load\n",
        "bertscore = load(\"bertscore\")\n",
        "\n",
        "benchmark_data = pd.read_csv(\"../p-eval-set/seen-data.csv\")\n",
        "\n",
        "for idx, row in benchmark_data.iterrows():\n",
        "    user_query = row[\"Question\"]\n",
        "    correct_answer = row[\"Answer\"]\n",
        "\n",
        "    responses = generate_answer(user_query)\n",
        "    \n",
        "    predictions = [responses.get(\"Response\", \"N/A\")]\n",
        "    references = [correct_answer]\n",
        "    results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Benchmark Query {idx + 1}: {user_query}\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\\nRAG Response:\\n\", responses.get(\"Response\", \"N/A\"))\n",
        "    print(\"\\n(Benchmark) Answer:\\n\", correct_answer)\n",
        "    print(\"BERT Score == \", results)\n",
        "    print(\"=\"*50 + \"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CampusConnect",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
