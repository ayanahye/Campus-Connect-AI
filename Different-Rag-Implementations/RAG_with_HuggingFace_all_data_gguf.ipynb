{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08CJHqgU4QVR"
      },
      "source": [
        "Updated notebook - main changes: revert to separating each category into its own collection, implement 2 step pipeline (categorize, then retrieve from those relevant collections). Somehow, takes a substantial amount of time now (ie 7 queries in 17 minutes) (using general qa set and some other random set of data), to discuss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNlcwxKR9gXt"
      },
      "source": [
        "Install some dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0wlrXoS1EP-"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U accelerate==0.27.1\n",
        "!pip install -q -U datasets==2.17.0\n",
        "!pip install -q -U transformers==4.38.1\n",
        "!pip install langchain sentence-transformers chromadb langchainhub\n",
        "\n",
        "!pip install langchain-community langchain-core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DedwIx35-UcR"
      },
      "source": [
        "Get the Model You Want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu5dP1rbDv0E"
      },
      "outputs": [],
      "source": [
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaDCl4Jj-YCQ"
      },
      "source": [
        "Define Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymTffeGN4EH-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/CampusConnect/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "model_path = \"mistral-7b-instruct-v0.2.Q4_0.gguf\"\n",
        "model = Llama(model_path=model_path, n_ctx=2048, n_threads=8, verbose=False)\n",
        "\n",
        "# repo_id = \"google/gemma-2-2b-it\"\n",
        "\n",
        "# hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "# # obv params, max_length is max token len for generated text, temp=0.1 means give more predictable and less random results\n",
        "# model = HuggingFaceEndpoint(\n",
        "#     task='text-generation',\n",
        "#     repo_id=repo_id,\n",
        "#     max_length=1024,\n",
        "#     temperature=0.1,\n",
        "#     huggingfacehub_api_token=hf_token\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGq9g41dv3ru"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/fz/m3b664vj5hq4kdmwr645k9_h0000gn/T/ipykernel_75494/3407937140.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import chromadb\n",
        "\n",
        "# pt model for generating embeddings used pretty often\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# persistent client to interact w chroma vector store\n",
        "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# create collections for each data (for testing rn)\n",
        "collection = client.get_or_create_collection(name=\"combined_docs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBA-ljP5-bOR"
      },
      "source": [
        "Define Data Sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVLl-QTT61Aw",
        "outputId": "22d88b44-33d8-4f08-c0d1-2d480c1944ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "study_permit_general: Loaded 14 docs.\n",
            "work_permit_student_general: Loaded 23 docs.\n",
            "work-study-data-llm: Loaded 178 docs.\n",
            "vancouver_transit_qa_pairs: Loaded 37 docs.\n",
            "permanent_residence_student_general: Loaded 6 docs.\n",
            "data-with-sources: Loaded 75 docs.\n",
            "faq_qa_pairs_general: Loaded 220 docs.\n",
            "hikes_qa: Loaded 278 docs.\n",
            "sfu-faq-with-sources: Loaded 102 docs.\n",
            "sfu-housing-with-sources: Loaded 74 docs.\n",
            "sfu-immigration-faq: Loaded 83 docs.\n",
            "park_qa_pairs-up: Loaded 216 docs.\n",
            "cultural_space_qa_pairs_up: Loaded 560 docs.\n",
            "qa_pairs_food: Loaded 38 docs.\n",
            "qa_pairs_year_and_month_avg: Loaded 1488 docs.\n",
            "qa_pairs_sfu_clubs: Loaded 163 docs.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "import uuid\n",
        "import os\n",
        "\n",
        "file_names = [\n",
        "    \"study_permit_general\", \"work_permit_student_general\", \"work-study-data-llm\",\n",
        "    \"vancouver_transit_qa_pairs\", \"permanent_residence_student_general\", \"data-with-sources\",\n",
        "    \"faq_qa_pairs_general\", \"hikes_qa\", \"sfu-faq-with-sources\", \"sfu-housing-with-sources\",\n",
        "    \"sfu-immigration-faq\", \"park_qa_pairs-up\", \"cultural_space_qa_pairs_up\",\n",
        "    \"qa_pairs_food\", \"qa_pairs_year_and_month_avg\", \"qa_pairs_sfu_clubs\"\n",
        "]\n",
        "\n",
        "collections = {}\n",
        "batch_size = 32\n",
        "\n",
        "def process_file(file):\n",
        "    try:\n",
        "        path = f'../Data/{file}.csv'\n",
        "        if not os.path.exists(path):\n",
        "            return f\"{file} skipped (file not found).\"\n",
        "\n",
        "        df = pd.read_csv(path, usecols=lambda col: col.lower() in {\"question\", \"answer\"})\n",
        "        df.columns = df.columns.str.lower()\n",
        "\n",
        "        if \"question\" not in df.columns or \"answer\" not in df.columns:\n",
        "            return f\"{file} skipped (missing question/answer columns).\"\n",
        "\n",
        "        df = df.drop_duplicates(subset=\"question\")\n",
        "        df[\"text\"] = df[\"question\"].fillna('') + ' ' + df[\"answer\"].fillna('')\n",
        "        unique_texts = list(set(df[\"text\"].dropna().tolist()))\n",
        "\n",
        "        collection = client.get_or_create_collection(name=file)\n",
        "        for i in range(0, len(unique_texts), batch_size):\n",
        "            batch = unique_texts[i:i + batch_size]\n",
        "            embeddings = embedding_model.embed_documents(batch)\n",
        "            ids = [str(uuid.uuid4()) for _ in batch]\n",
        "            collection.add(ids=ids, embeddings=embeddings, documents=batch)\n",
        "\n",
        "        collections[file] = collection\n",
        "        return f\"{file}: Loaded {len(unique_texts)} docs.\"\n",
        "    except Exception as e:\n",
        "        return f\"{file}: Error - {e}\"\n",
        "\n",
        "# parallelogram\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
        "    results = list(executor.map(process_file, file_names))\n",
        "\n",
        "for result in results:\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r976Yrgqdvjk"
      },
      "outputs": [],
      "source": [
        "# define cat to collection mapping\n",
        "# motivation: takes wayyyy too long now -> dataset size trippled and time grew exponentially...\n",
        "# now takes around 2 mins on avg for each response..compared to 40 seconds previously..\n",
        "\n",
        "collection_map = {\n",
        "    \"study\": \"study_permit_general\",\n",
        "    \"student work\": \"work_permit_student_general\",\n",
        "    \"work-study\": \"work-study-data-llm\",\n",
        "    \"transit\": \"vancouver_transit_qa_pairs\",\n",
        "    \"permanent residence\": \"permanent_residence_student_general\",\n",
        "    \"general info\": \"data-with-sources\",\n",
        "    \"faq\": \"faq_qa_pairs_general\",\n",
        "    \"hiking\": \"hikes_qa\",\n",
        "    \"sfu faq\": \"sfu-faq-with-sources\",\n",
        "    \"housing\": \"sfu-housing-with-sources\",\n",
        "    \"immigration\": \"sfu-immigration-faq\",\n",
        "    \"parks\": \"park_qa_pairs-up\",\n",
        "    \"culture\": \"cultural_space_qa_pairs_up\",\n",
        "    \"food\": \"qa_pairs_food\",\n",
        "    \"weather\": \"qa_pairs_year_and_month_avg\",\n",
        "    \"clubs\": \"qa_pairs_sfu_clubs\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn2LQiJq_k9t"
      },
      "source": [
        "Function to now match for releveant document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEulNQZw8mDo"
      },
      "outputs": [],
      "source": [
        "def get_relevant_documents(query, categories, n_results=3):\n",
        "    all_results = []\n",
        "    query_embedding = embedding_model.embed_documents([query])[0]\n",
        "\n",
        "    for category in categories:\n",
        "        collection_name = collection_map[category]\n",
        "        if collection_name in collections:\n",
        "            try:\n",
        "                result = collections[collection_name].query(\n",
        "                    query_embeddings=[query_embedding],\n",
        "                    n_results=n_results\n",
        "                )\n",
        "                docs = result.get(\"documents\", [[]])[0]\n",
        "                sims = result.get(\"distances\", [[]])[0]\n",
        "\n",
        "                all_results.extend(zip(docs, sims))\n",
        "            except Exception as e:\n",
        "                print(f\"error querying {collection_name}: {e}\")\n",
        "\n",
        "    all_results = sorted(all_results, key=lambda x: x[1])\n",
        "\n",
        "    return all_results[:n_results]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjivdEV5fPxq"
      },
      "source": [
        "### Classify Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVTQznmhfN9V"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import difflib\n",
        "\n",
        "valid_categories = list(collection_map.keys())\n",
        "fallback_category = \"faq\"\n",
        "\n",
        "def classify_query(query):\n",
        "    category_prompt = f\"\"\"\n",
        "    You are a classifier for a Q&A system for international students in British Columbia.\n",
        "    Choose the **1 most relevant** category from this list, or at most 3 if absolutely needed (comma-separated):\n",
        "\n",
        "    {\", \".join(valid_categories)}\n",
        "\n",
        "    Query: \"{query}\"\n",
        "\n",
        "    Return only the category name(s) as a comma-separated string.\n",
        "    \"\"\"\n",
        "\n",
        "    response = model(category_prompt, max_tokens=50, temperature=0.1)[\"choices\"][0][\"text\"].strip().lower()\n",
        "    print(\"Raw out:\", response)\n",
        "\n",
        "    tokens = re.findall(r'\\b\\w+\\b', response)\n",
        "\n",
        "    matched = []\n",
        "    for token in tokens:\n",
        "        closest = difflib.get_close_matches(token, valid_categories, n=1, cutoff=0.8)\n",
        "        if closest and closest[0] not in matched:\n",
        "            matched.append(closest[0])\n",
        "        if len(matched) == 3:\n",
        "            break\n",
        "\n",
        "    if fallback_category not in matched:\n",
        "      matched.append(fallback_category)\n",
        "\n",
        "    return matched[:3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV_h8UJa_pGj"
      },
      "source": [
        "Generate Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMTYliNl8onj"
      },
      "outputs": [],
      "source": [
        "def generate_answer(query):\n",
        "    categories = classify_query(query)\n",
        "    print(f\"Categories {categories}\\n\")\n",
        "    relevant_documents = get_relevant_documents(query, categories)\n",
        "\n",
        "    if not relevant_documents:\n",
        "        return {\n",
        "            \"Response\": \"Sorry, no relevant documents found.\"\n",
        "        }\n",
        "\n",
        "    #relevant_documents = list(set(relevant_documents))\n",
        "\n",
        "    seen = set()\n",
        "    unique_docs = []\n",
        "    for doc, sim in relevant_documents:\n",
        "        if doc not in seen:\n",
        "            seen.add(doc)\n",
        "            unique_docs.append((doc, sim))\n",
        "\n",
        "    print(\"Relevant Documents with Similarity Scores:\")\n",
        "    for doc, sim in unique_docs:\n",
        "        print(f\"Similarity: {sim:.4f}\\nDoc: {doc}\\n\")\n",
        "\n",
        "    relevant_texts = \"\\n\\n\".join([doc for doc, _ in unique_docs])\n",
        "\n",
        "    rag_prompt = f\"\"\"\n",
        "    You are a helpful, friendly assistant for international students new to British Columbia, Canada.\n",
        "\n",
        "    Below are some reference documents that may be relevant to the user's question:\n",
        "    {relevant_texts}\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    1. If the user's query is just a greeting (like \"hello\", \"hi\", \"what's up\"):\n",
        "       - Respond with a single brief friendly greeting\n",
        "       - Offer to help with questions about studying or living in BC\n",
        "       - Do NOT include ANY information from the reference documents\n",
        "       - Do NOT create additional questions and answers\n",
        "\n",
        "    2. If the user is asking for information:\n",
        "       - Be friendly and answer based ONLY on the reference documents if relevant\n",
        "       - Keep your answer brief and concise (under 30 words when possible)\n",
        "       - If the documents don't provide sufficient information, say \"I don't have enough information to answer that. Please refer to official sources.\"\n",
        "       - Do NOT create additional questions and answers beyond answering their original question\n",
        "\n",
        "    3. IMPORTANT: Never generate additional content beyond answering the user's question\n",
        "\n",
        "    User question: {query}\n",
        "\n",
        "    Your response (just the answer, no preamble):\n",
        "    \"\"\"\n",
        "\n",
        "    response_after_rag = model(rag_prompt, max_tokens=300, temperature=0.1)[\"choices\"][0][\"text\"]\n",
        "    # response_after_rag = model(rag_prompt, max_tokens=300, temperature=0.1)\n",
        "\n",
        "    return {\n",
        "        \"Response\": response_after_rag\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxtEFTiK_q8E"
      },
      "source": [
        "Example Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw out: answer: transit\n",
            "Categories ['transit', 'faq']\n",
            "\n",
            "Relevant Documents with Similarity Scores:\n",
            "Similarity: 27.6120\n",
            "Doc: How do I pay for the bus in Vancouver? In Vancouver, you pay for public transit using a Compass Card, which you can purchase at SkyTrain stations, London Drugs stores, and other locations. As an international student, you're likely eligible for the discounted U-Pass BC program through your institution.\n",
            "\n",
            "Similarity: 28.5323\n",
            "Doc: Do international students get discounts on Vancouver public transit? Yes, international students at recognized post-secondary institutions can get a discounted Compass Card (U-Pass BC) which provides unlimited transit use. This is typically included in your student fees.\n",
            "\n",
            "Similarity: 28.6289\n",
            "Doc: What is a U-Pass and how do I get one as an international student? A U-Pass BC is a discounted transit pass available to eligible students at participating post-secondary institutions in Metro Vancouver. As an international student, you typically receive this as part of your student fees. You'll need to link it to a Compass Card that you can obtain at any SkyTrain station or London Drugs store.\n",
            "\n",
            " The U-Pass BC is a discounted transit pass for eligible students at participating post-secondary institutions in Metro Vancouver. It provides unlimited transit use.\n"
          ]
        }
      ],
      "source": [
        "answer = generate_answer(\"What is the upass BC for students in vancouver transit?\")\n",
        "print(answer[\"Response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EVDilNoM8sU8",
        "outputId": "20b5ccda-84b1-4213-d257-dcd59e8e761c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'generate_answer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m user_query \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m correct_answer \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m(user_query)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBenchmark Query \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_answer' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "benchmark_data = pd.read_csv(\"../poorvi-test-QuesAns.csv\")\n",
        "\n",
        "for idx, row in benchmark_data.iterrows():\n",
        "    user_query = row[\"Question\"]\n",
        "    correct_answer = row[\"Answer\"]\n",
        "\n",
        "    responses = generate_answer(user_query)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Benchmark Query {idx + 1}: {user_query}\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\\nRAG Response:\\n\", responses.get(\"Response\", \"N/A\"))\n",
        "    print(\"\\n(Benchmark) Answer:\\n\", correct_answer)\n",
        "    print(\"=\"*50 + \"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CampusConnect",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
