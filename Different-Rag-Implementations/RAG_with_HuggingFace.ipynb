{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym5dMYn-9Sum"
      },
      "source": [
        "## Hello, Here's How to use RAG w HF Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNlcwxKR9gXt"
      },
      "source": [
        "Install some dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q0wlrXoS1EP-",
        "outputId": "8b98c2f2-1383-4b44-feaa-6a1c06c96c11"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes==0.42.0\n",
        "!pip install -q -U peft==0.8.2\n",
        "!pip install -q -U trl==0.7.10\n",
        "!pip install -q -U accelerate==0.27.1\n",
        "!pip install -q -U datasets==2.17.0\n",
        "!pip install -q -U transformers==4.41.0\n",
        "!pip install langchain sentence-transformers chromadb langchainhub\n",
        "!pip install tensorflow\n",
        "!pip install tf-keras\n",
        "!pip install chromadb\n",
        "!pip install langchain-community langchain-core\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DedwIx35-UcR"
      },
      "source": [
        "Get the Model You Want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "M-dvCR_M1qQR"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# get the repository ID for the Gemma 2b model which I am testing with\n",
        "repo_id = \"google/gemma-2-2b-it\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaDCl4Jj-YCQ"
      },
      "source": [
        "Define Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymTffeGN4EH-",
        "outputId": "cc848d1e-8b63-41ce-ea1f-5f877e7e16f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# set your own hf token then fetch it here\n",
        "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "# obv params, max_length is max token len for generated text, temp=0.1 means give more predictable and less random results\n",
        "llm = HuggingFaceEndpoint(\n",
        "    task='text-generation',\n",
        "    repo_id=repo_id,\n",
        "    model=\"google/gemma-2-2b-it\",\n",
        "    max_length=1024,\n",
        "    temperature=0.1,\n",
        "    huggingfacehub_api_token=hf_token\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBA-ljP5-bOR"
      },
      "source": [
        "Define Data Sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "UVLl-QTT61Aw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load ur data\n",
        "health_data = pd.read_csv('sample_data/data-with-sources.csv')\n",
        "work_data = pd.read_csv('sample_data/work-and-education-data.csv')\n",
        "# transit_data = pd.read_csv('../Transit-Data-Ques-Ans/vancouver_transit_qa_pairs.csv')\n",
        "\n",
        "health_data_sample = health_data\n",
        "work_data_sample = work_data\n",
        "# transit_data_sample = transit_data\n",
        "\n",
        "health_data_sample['text'] = health_data_sample['Question'].fillna('') + ' ' + health_data_sample['Answer'].fillna('')\n",
        "work_data_sample['text'] = work_data_sample['Theme'].fillna('') + ' ' + work_data_sample['Content'].fillna('')\n",
        "# transit_data_sample['text'] = transit_data_sample['question'].fillna('') + ' ' + transit_data_sample['answer'].fillna('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To Delete Collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory permissions: 755\n",
            "Permissions updated\n",
            "Existing collections: ['health_docs', 'work_docs', 'study_docs']\n",
            "Deleted health_docs collection\n",
            "Deleted work_docs collection\n",
            "Error deleting transit_docs: Collection transit_docs does not exist.\n"
          ]
        }
      ],
      "source": [
        "# # Print the permissions of your database directory\n",
        "# db_path = \"./chroma_db\"\n",
        "# print(f\"Directory permissions: {oct(os.stat(db_path).st_mode)[-3:]}\")\n",
        "\n",
        "# # Try to make it writable\n",
        "# try:\n",
        "#     os.chmod(db_path, 0o755)  # rwxr-xr-x\n",
        "#     # Also make the files inside writable\n",
        "#     for root, dirs, files in os.walk(db_path):\n",
        "#         for d in dirs:\n",
        "#             os.chmod(os.path.join(root, d), 0o755)\n",
        "#         for f in files:\n",
        "#             os.chmod(os.path.join(root, f), 0o644)  # rw-r--r--\n",
        "#     print(\"Permissions updated\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error changing permissions: {e}\")\n",
        "\n",
        "# existing_collections = client.list_collections()\n",
        "# print(f\"Existing collections: {existing_collections}\")\n",
        "\n",
        "# client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# # Delete collections if they exist\n",
        "# try:\n",
        "#     client.delete_collection(\"health_docs\")\n",
        "#     print(\"Deleted health_docs collection\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error deleting health_docs: {e}\")\n",
        "\n",
        "# try:\n",
        "#     client.delete_collection(\"work_docs\")\n",
        "#     print(\"Deleted work_docs collection\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error deleting work_docs: {e}\")\n",
        "    \n",
        "# try:\n",
        "#     client.delete_collection(\"transit_docs\")\n",
        "#     print(\"Deleted transit_docs collection\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error deleting transit_docs: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5NPcS-__W2F"
      },
      "source": [
        "Set Embedding Model, and Chroma Client to Interact w Vector Database and Create Collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "JKdxYOca7KfJ"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import chromadb\n",
        "\n",
        "# pt model for generating embeddings used pretty often\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\" ## currently the best one found \n",
        ")\n",
        "\n",
        "## embeddings and time it takes \n",
        "# sentence-transformers/all-MiniLM-L6-v2 = 32.7 seconds\n",
        "# sentence-transformers/paraphrase-MiniLM-L6-v2 = 30.5 seconds !!!!!!!!\n",
        "# sentence-transformers/all-roberta-large-v1 = 1m 55.8 seconds \n",
        "# sentence-transformers/all-MiniLM-L12-v2 = 54.4 seconds \n",
        "# sentence-transformers/multi-qa-MiniLM-L6-cos-v1 = 49.5 seconds \n",
        "# sentence-transformers/paraphrase-mpnet-base-v2 = over 3 minutes \n",
        "# sentence-transformers/multi-qa-mpnet-base-dot-v1 = 2m 18.7 seconds\n",
        "# \"neuml/pubmedbert-base-embeddings\" = 2m 27.4 seconds \n",
        "\n",
        "# persistent client to interact w chroma vector store\n",
        "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# create collections for each data (for testing rn)\n",
        "health_collection = client.get_or_create_collection(name=\"health_docs\")\n",
        "work_collection = client.get_or_create_collection(name=\"work_docs\")\n",
        "# transit_collection = client.get_or_create_collection(name=\"transit_docs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOEGkLgl_e2Z"
      },
      "source": [
        "Function to add data to collection by embedding them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lN1Q5d_8jZt",
        "outputId": "5d2003a4-e061-434b-ee7c-c0c852f47c06"
      },
      "outputs": [],
      "source": [
        "def add_data_to_collection(collection, data):\n",
        "    for idx, row in data.iterrows():\n",
        "        try:\n",
        "            # get the embeddings using the embedding model for the documents\n",
        "            embeddings = embedding_model.embed_documents([row['text']])[0]\n",
        "            collection.add(\n",
        "                ids=[str(idx)],\n",
        "                embeddings=[embeddings],\n",
        "                documents=[row['text']]\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error on index {idx}: {e}\")\n",
        "\n",
        "# add data to collections\n",
        "add_data_to_collection(health_collection, health_data_sample)\n",
        "add_data_to_collection(work_collection, work_data_sample)\n",
        "# add_data_to_collection(transit_collection, transit_data_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn2LQiJq_k9t"
      },
      "source": [
        "Function to now match for releveant document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "TEulNQZw8mDo"
      },
      "outputs": [],
      "source": [
        "def get_relevant_document(query, category):\n",
        "    try:\n",
        "        # get the embedding for the user query using same embedding model\n",
        "        query_embeddings = embedding_model.embed_documents([query])[0]\n",
        "\n",
        "        # choose the correct collection based on the category\n",
        "        if category == \"health\":\n",
        "            collection = health_collection\n",
        "        elif category == \"work\":\n",
        "            collection = work_collection\n",
        "        # elif category == \"transit\":\n",
        "            # collection = transit_collection\n",
        "        # collection = health_collection if category == \"health\" else work_collection\n",
        "\n",
        "        # query the collection\n",
        "        results = collection.query(query_embeddings=[query_embeddings], n_results=1)\n",
        "\n",
        "        print(f\"Query Results: {results}\")\n",
        "\n",
        "        return results['documents'][0][0] if results['documents'] else None\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV_h8UJa_pGj"
      },
      "source": [
        "Generate Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "EMTYliNl8onj"
      },
      "outputs": [],
      "source": [
        "def generate_answer(query, category):\n",
        "    # b4 rag\n",
        "    output_before_rag = llm.predict(f\"Respond to this question: {query}\")\n",
        "    response_before_rag = output_before_rag\n",
        "\n",
        "    # get the relevant document\n",
        "    relevant_document = get_relevant_document(query, category)\n",
        "    if relevant_document is None:\n",
        "        return f\"Sorry, no relevant document found. Model's response before RAG: {response_before_rag}\"\n",
        "\n",
        "    relevant_document = \" \".join(relevant_document.split())\n",
        "    MAX_DOC_LENGTH = 500\n",
        "    relevant_document = relevant_document[:MAX_DOC_LENGTH]\n",
        "\n",
        "    # rag_prompt = f\"\"\"\n",
        "    # You are a helpful assistant for international students new to B.C. Here is a relevant document:\n",
        "\n",
        "    # {relevant_document}\n",
        "\n",
        "    # Please respond to the following question based on the document above:\n",
        "\n",
        "    # Question: {query}\n",
        "\n",
        "    # Answer:\n",
        "    # \"\"\"\n",
        "    rag_prompt = f\"\"\"\n",
        "    You are a helpful assistant for international students new to B.C. Here is a relevant document:\n",
        "\n",
        "    {relevant_document}\n",
        "\n",
        "    Please respond to the following question based on the document above, if you can't answer anything or it requires the international student to ask a query again, direct them to additional resources like the vancouver transit website or the transit mobile app for transit related queries:\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # print(\"Prompt being sent to model:\")\n",
        "    # print(rag_prompt)\n",
        "\n",
        "    # now generate using RAG\n",
        "    output_after_rag = llm.predict(rag_prompt)\n",
        "    # print(\"Output from model:\", output_after_rag)\n",
        "\n",
        "    response_after_rag = output_after_rag\n",
        "\n",
        "    # return both responses to compare\n",
        "    return {\n",
        "        \"Before RAG Response\": response_before_rag,\n",
        "        \"After RAG Response\": response_after_rag\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxtEFTiK_q8E"
      },
      "source": [
        "Example Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVDilNoM8sU8",
        "outputId": "d1198887-bed2-455b-b25e-61c3a084d089"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/CampusConnect/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query Results: {'ids': [['1']], 'embeddings': None, 'documents': [['Who qualifies for MSP coverage? A B.C. resident who is a Canadian citizen or lawfully admitted to Canada for permanent residence, makes their home in B.C., and is physically present in B.C. for at least six months in a calendar year.']], 'uris': None, 'data': None, 'metadatas': [[None]], 'distances': [[16.354888726841512]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/CampusConnect/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Query: What do I need to do to apply for MSP coverage in B.C.?\n",
            "Response Before RAG: \n",
            "\n",
            "Here's a breakdown of the process:\n",
            "\n",
            "**1. Determine Eligibility:**\n",
            "   * **Work:** Are you employed in a qualifying job?\n",
            "   * **Income:** Do you meet the income requirements?\n",
            "   * **Health:** Do you have a pre-existing condition?\n",
            "\n",
            "**2. Apply for Coverage:**\n",
            "   * **Online:** Visit the WorkSafeBC website and complete the online application.\n",
            "   * **By Phone:** Call WorkSafeBC at 1-888-328-2222.\n",
            "   * **In Person:** Visit a WorkSafeBC office.\n",
            "\n",
            "**3. Provide Documentation:**\n",
            "   * **Proof of Employment:** Pay stubs, employment contract, etc.\n",
            "   * **Proof of Income:** Tax returns, bank statements, etc.\n",
            "   * **Proof of Health:** Medical records, doctor's notes, etc.\n",
            "\n",
            "**4. Pay Premiums:**\n",
            "   * **Monthly:** Premiums are typically paid monthly.\n",
            "   * **Payment Options:** WorkSafeBC offers various payment options.\n",
            "\n",
            "**5. Receive Coverage:**\n",
            "   * **Confirmation:** You will receive confirmation of your coverage.\n",
            "   * **Benefits:** You can access benefits once your coverage is active.\n",
            "\n",
            "**Important Notes:**\n",
            "* **Eligibility requirements:** WorkSafeBC has specific eligibility requirements for MSP coverage.\n",
            "* **Waiting period:** There may be a waiting period before you can access benefits.\n",
            "* **Coverage options:** WorkSafeBC offers different coverage options, including basic, comprehensive, and extended coverage.\n",
            "\n",
            "\n",
            "**Additional Resources:**\n",
            "* **WorkSafeBC website:** https://www.worksafebc.ca/\n",
            "* **WorkSafeBC phone number:** 1-888-328-2222\n",
            "\n",
            "\n",
            "This is a general overview of the process. For specific details and personalized advice, it's best to contact WorkSafeBC directly. \n",
            "\n",
            "Response After RAG: To apply for MSP coverage in B.C., you need to meet the following requirements:\n",
            "    * Be a Canadian citizen or lawfully admitted to Canada for permanent residence.\n",
            "    * Make your home in B.C.\n",
            "    * Be physically present in B.C. for at least six months in a calendar year.\n",
            "\n",
            "    You can apply online at [link to online application]. \n",
            "    \n",
            "    **Note:** This information is for general guidance only. For specific details and eligibility criteria, please refer to the official website of the Ministry of Health. \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "    \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# user_query = \"How do I commute in vancouver and how can I get to SFU?\"\n",
        "user_query = \"What do I need to do to apply for MSP coverage in B.C.?\"\n",
        "# category = \"transit\"\n",
        "category = \"health\"\n",
        "responses = generate_answer(user_query, category)\n",
        "\n",
        "print(\"User Query:\", user_query)\n",
        "print(\"Response Before RAG:\", responses[\"Before RAG Response\"])\n",
        "print(\"Response After RAG:\", responses[\"After RAG Response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXMwMcnvAWz_",
        "outputId": "ffe2ba21-49e0-49d3-a490-c2ea767da941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents in health collection: 76\n",
            "Number of documents in work collection: 878\n"
          ]
        }
      ],
      "source": [
        "# verify\n",
        "health_docs = health_collection.get()\n",
        "print(\"Number of documents in health collection:\", len(health_docs['documents']))\n",
        "\n",
        "work_docs = work_collection.get()\n",
        "print(\"Number of documents in work collection:\", len(work_docs['documents']))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CampusConnect",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
